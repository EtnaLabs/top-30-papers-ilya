order,date,title,authors,link,summary,takeaways,slide3,slide4
1,01 Aug 2017,"The Annotated Transformer","Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin",https://nlp.seas.harvard.edu/annotated-transformer/,"This paper is a simplified and annotated version of the Attention Is All You Need paper, it shows the architecture of the Transformer model, with diagrams and python code.","Bullets: Offers a practical, executable implementation of the Transformer architecture. | Breaks down complex concepts with detailed annotations and explanations. | Serves as an educational bridge between theoretical papers and practical implementation.","Title: Transformer Architecture | Image: https://miro.medium.com/v2/resize:fit:640/format:webp/1*InsTuWpZTYm0kwi8ovIMAQ.png",""
2,23 Sep 2011,"The First Law of Complexodynamics","Scott Aaronson",https://scottaaronson.blog/?p=762,"This blog post explores the dynamics of complexity in closed systems using the Coffee Automaton model. It analyzes how complexity emerges, evolves, and dissipates, providing insights into the fundamental principles governing complex systems and their applications in various scientific fields.","Bullets: Analyzes the dynamics of complexity in closed systems using the Coffee Automaton model. | Provides insights into the mechanisms driving complexity changes over time. | Offers a framework for predicting complexity trends in similar systems.","Image: https://149663533.v2.pressablecdn.com/coffee-small.jpg","Image: https://scottaaronson.blog/complexity-lrg.jpg"
3,21 May 2015,"The Unreasonable Effectiveness of Recurrent Neural Networks","Andrej Karpathy",https://karpathy.github.io/2015/05/21/rnn-effectiveness/,"This paper highlights the surprising capabilities of recurrent neural networks in sequence prediction tasks. It demonstrates their effectiveness in generating coherent sequences, from text to music, showcasing the potential of RNNs in creative and predictive applications.","Bullets: Highlights the surprising capabilities of recurrent neural networks in various applications. | Explores the reasons behind the effectiveness of RNNs in sequence modeling. | Discusses the potential and limitations of RNNs in real-world scenarios.","",""
4,27 Aug 2015,"Understanding LSTM Networks","Christopher Olah",https://colah.github.io/posts/2015-08-Understanding-LSTMs/,"This paper explains Long Short-Term Memory (LSTM) networks, a type of recurrent neural network capable of learning long-term dependencies. It covers their architecture, functionality, and applications, highlighting their effectiveness in sequence prediction tasks across various domains.","Bullets: Provides an in-depth explanation of Long Short-Term Memory (LSTM) networks. | Clarifies the mechanisms that enable LSTMs to capture long-range dependencies. | Offers practical guidance for implementing and training LSTM models.","",""
5,08 Sep 2014,"Recurrent Neural Network Regularization","Wojciech Zaremba; Ilya Sutskever; Oriol Vinyals",https://arxiv.org/pdf/1409.2329.pdf,"This paper explores techniques to regularize recurrent neural networks, addressing overfitting issues. It introduces methods like dropout and weight noise, improving model generalization and performance. These techniques are crucial for enhancing the robustness of RNNs in various applications.","Bullets: Proposes new regularization techniques to enhance the performance of recurrent neural networks. | Reduces overfitting and improves generalization in RNN models. | Validates the effectiveness of the methods through extensive experiments.","",""
6,01 Jul 1993,"Keeping Neural Networks Simple by Minimizing the Description Length of the Weights","Geoffrey E. Hinton; Drew van Camp",https://www.cs.toronto.edu/~hinton/absps/colt93.pdf,"Hinton & van Camp reduce overfitting in neural networks by minimizing weight description length using the MDL principle. This method simplifies to L2 weight-decay with a Gaussian prior, linking MDL to Bayesian regularization. It supports weight-sharing, pruning, and adaptive noise, effectively improving test errors.","Bullets: Simplifies neural network models by reducing the complexity of weight descriptions. | Enhances model generalization by focusing on essential features. | Reduces overfitting by minimizing unnecessary parameters.","",""
7,09 Jun 2015,"Pointer Networks","Oriol Vinyals; Meire Fortunato; Navdeep Jaitly",https://arxiv.org/pdf/1506.03134.pdf,"This paper introduces Pointer Networks, a novel neural architecture for solving combinatorial optimization problems. It uses attention mechanisms to select output sequences, outperforming traditional methods in tasks like sorting and routing, and offering new solutions in optimization challenges.","Bullets: Introduces Pointer Networks, a novel neural architecture for solving combinatorial problems. | Demonstrates the model's ability to handle variable-sized outputs. | Validates the approach on tasks like sorting and the traveling salesman problem.","",""
8,01 Dec 2012,"ImageNet Classification with Deep Convolutional Neural Networks","Alex Krizhevsky; Ilya Sutskever; Geoffrey E. Hinton",https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf,"AlexNet transformed image classification with deep convolutional neural networks, achieving high accuracy on ImageNet. It introduced ReLU activations, dropout, and GPU use, significantly advancing computer vision and deep learning architectures.","Bullets: Demonstrates the effectiveness of deep CNNs in large-scale image classification. | Highlights the role of data augmentation and dropout in improving model performance. | Sets a new benchmark for image recognition tasks with significant accuracy improvements.","",""
9,19 Nov 2015,"Order Matters: Sequence-to-sequence for sets","Oriol Vinyals; Samy Bengio; Manjunath Kudlur",https://arxiv.org/pdf/1511.06391.pdf,"This paper explores the importance of order in sequence-to-sequence models for set-based tasks. It demonstrates how different orderings can impact model performance, providing insights into optimizing sequence processing for improved accuracy in set-related applications.","Bullets: Investigates the impact of input order on sequence-to-sequence models for set-based tasks. | Proposes methods to mitigate order sensitivity in model training. | Demonstrates improved performance on tasks where order invariance is crucial.","",""
10,16 Nov 2018,"GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism","Yanping Huang; Youlong Cheng; Ankur Bapna; Orhan Firat; Mia Xu Chen; Dehao Chen; HyoukJoong Lee; Jiquan Ngiam; Quoc V. Le; Yonghui Wu; Zhifeng Chen",https://arxiv.org/pdf/1811.06965.pdf,"This paper presents GPipe, a method for training large neural networks efficiently using pipeline parallelism. It improves scalability and reduces training time, enabling the handling of giant models in deep learning applications.","Bullets: Introduces pipeline parallelism to efficiently train large neural networks. | Reduces memory usage by partitioning models across multiple accelerators. | Demonstrates scalability and improved training speed for giant models.","",""
11,10 Dec 2015,"Deep Residual Learning for Image Recognition","Kaiming He; Xiangyu Zhang; Shaoqing Ren; Jian Sun",https://arxiv.org/pdf/1512.03385.pdf,"This paper introduces deep residual networks (ResNets) for image recognition. It addresses the degradation problem in deep networks by using residual learning, significantly improving accuracy and enabling the training of much deeper models.","Bullets: Introduces residual learning framework to ease training of deep networks. | Significantly improves accuracy in image recognition tasks. | Enables the training of extremely deep networks with hundreds of layers.","",""
12,23 Nov 2015,"Multi-Scale Context Aggregation by Dilated Convolutions","Fisher Yu; Vladlen Koltun",https://arxiv.org/pdf/1511.07122.pdf,"This paper introduces dilated convolutions for multi-scale context aggregation in neural networks. It enhances feature extraction by expanding the receptive field without increasing parameters, improving performance in tasks like image segmentation and object detection.","Bullets: Introduces dilated convolutions for effective multi-scale context aggregation. | Enhances feature extraction without increasing computational cost. | Improves performance in dense prediction tasks like semantic segmentation.","",""
13,04 Apr 2017,"Neural Message Passing for Quantum Chemistry","Justin Gilmer; Samuel S. Schoenholz; Patrick F. Riley; Oriol Vinyals; George E. Dahl",https://arxiv.org/pdf/1704.01212.pdf,"This paper presents a neural message passing framework for quantum chemistry, enhancing molecular property prediction. It leverages graph neural networks to model interactions, improving accuracy and efficiency in computational chemistry tasks.","Bullets: Develops a neural message passing framework for molecular property prediction. | Achieves state-of-the-art results in quantum chemistry tasks. | Offers a scalable approach to model complex molecular interactions.","",""
14,12 Jun 2017,"Attention Is All You Need","Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N. Gomez; Łukasz Kaiser; Illia Polosukhin",https://arxiv.org/pdf/1706.03762.pdf,"This paper introduces the Transformer model, revolutionizing natural language processing by using self-attention mechanisms. It eliminates recurrence, improving parallelization and performance in sequence transduction tasks, setting new standards in NLP.","Bullets: Introduces the Transformer model, relying solely on attention mechanisms. | Achieves superior performance in machine translation tasks. | Simplifies the architecture by removing recurrence and convolutions.","",""
15,01 Sep 2014,"Neural Machine Translation by Jointly Learning to Align and Translate","Dzmitry Bahdanau; Kyunghyun Cho; Yoshua Bengio",https://arxiv.org/pdf/1409.0473.pdf,"This paper introduces a novel approach to neural machine translation by integrating alignment and translation processes. It enhances translation accuracy by jointly learning these tasks, significantly improving performance over traditional methods, and setting a new standard in machine translation research.","Bullets: Introduces a novel approach to neural machine translation by integrating alignment and translation processes. | Improves translation accuracy by leveraging joint learning techniques. | Demonstrates the effectiveness of the model on various language pairs.","",""
16,16 Mar 2016,"Identity Mappings in Deep Residual Networks","Kaiming He; Xiangyu Zhang; Shaoqing Ren; Jian Sun",https://arxiv.org/pdf/1603.05027.pdf,"","","",""
17,05 Jun 2017,"A Simple Neural Network Module for Relational Reasoning","Adam Santoro; David Raposo; David G.T. Barrett; Mateusz Malinowski; Razvan Pascanu; Peter Battaglia; Timothy Lillicrap",https://arxiv.org/pdf/1706.01427.pdf,"","","",""
18,08 Nov 2016,"Variational Lossy Autoencoder","Xi Chen; Diederik P. Kingma; Tim Salimans; Yan Duan; Prafulla Dhariwal; John Schulman; Ilya Sutskever; Pieter Abbeel",https://arxiv.org/pdf/1611.02731.pdf,"","","",""
19,05 Jun 2018,"Relational Recurrent Neural Networks","Adam Santoro; Ryan Faulkner; David Raposo; Jack Rae; Mike Chrzanowski; Theophane Weber; Daan Wierstra; Oriol Vinyals; Razvan Pascanu; Timothy Lillicrap",https://arxiv.org/pdf/1806.01822.pdf,"","","",""
20,27 May 2014,"Quantifying the Rise and Fall of Complexity in Closed Systems: The Coffee Automaton","Scott Aaronson, Sean M. Carroll, Lauren Ouellette",https://arxiv.org/pdf/1405.6903,"","","",""
21,20 Oct 2014,"Neural Turing Machines","Alex Graves; Greg Wayne; Ivo Danihelka",https://arxiv.org/pdf/1405.6903.pdf,"","","",""
22,08 Dec 2015,"Deep Speech 2: End-to-End Speech Recognition in English and Mandarin","Dario Amodei; Rishita Anubhai; Eric Battenberg; Carl Case; Jared Casper; Bryan Catanzaro; Jingdong Chen; Mike Chrzanowski; Adam Coates; Greg Diamos; Erich Elsen; Jesse Engel; Linxi Fan; Christopher Fougner; Tony Han; Awni Hannun; Billy Jun; Patrick LeGresley; Libby Lin; Sharan Narang; Andrew Ng; Sherjil Ozair; Ryan Prenger; Jonathan Raiman; Sanjeev Satheesh; David Seetapun; Shubho Sengupta; Yi Wang; Zhiqian Wang; Chong Wang; Bo Xiao; Dani Yogatama; Jun Zhan; Zhenyao Zhu",https://arxiv.org/pdf/1512.02595.pdf,"","","",""
23,23 Jan 2020,"Scaling Laws for Neural Language Models","Jared Kaplan; Sam McCandlish; Tom Henighan; Tom B. Brown; Benjamin Chess; Rewon Child; Scott Gray; Alec Radford; Jeffrey Wu; Dario Amodei",https://arxiv.org/pdf/2001.08361.pdf,"","","",""
24,04 Jun 2004,"A Tutorial Introduction to the Minimum Description Length Principle","Peter Grunwald",https://arxiv.org/pdf/math/0406077.pdf,"","","",""
25,22 Jun 2008,"Machine Super Intelligence","Shane Legg",https://www.vetta.org/documents/Machine_Super_Intelligence.pdf,"","","",""
26,01 Jan 2008,"Kolmogorov Complexity and Algorithmic Randomness","A. Shen; V. A. Uspensky; N. Vereshchagin",https://www.lirmm.fr/~ashen/kolmbook-eng-scan.pdf,"","","",""
27,01 Jan 2015,"Stanford's CS231n Convolutional Neural Networks for Visual Recognition","Fei-Fei Li; Justin Johnson; Serena Yeung",https://cs231n.github.io/,"","","",""
